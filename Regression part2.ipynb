{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd59c9f-dd44-45c2-bd75-b5141e8c832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?\n",
    "\n",
    "R-squared (Coefficient of Determination) in Linear Regression:\n",
    "  R-squared, also known as the coefficient of determination, is a statistical measure that represents the proportion of the variance in the dependent variable that is explained by the independent variables in a linear regression model. It provides an indication of how well the independent variables explain the variability of the dependent variable.\n",
    "\n",
    "Calculation of R-squared:\n",
    "  The formula for calculating R-squared is as follows:\n",
    "    R^2 = 1-(Sum of Squared Residuals/Total Sum of Squares)\n",
    "Sum of Squared Residuals (SSR): This represents the sum of the squared differences between the observed values (actual values) of the dependent variable and the predicted values from the regression model.\n",
    "Total Sum of Squares (SST): This represents the sum of the squared differences between the observed values of the dependent variable and the mean of the dependent variable.\n",
    "The formula essentially compares the goodness of fit of the model to a model that simply predicts the mean of the dependent variable. A higher R-squared value indicates a better fit, as it implies that a larger proportion of the variance in the dependent variable is explained by the independent variables.\n",
    "\n",
    "Interpretation of R-squared:\n",
    "    \n",
    " R^2=0: The model does not explain any variability in the dependent variable.\n",
    " 0<R^2<1: The model explains a certain proportion of the variability in the dependent variable. A higher R-squared indicates a better fit.\n",
    " R^2 = 1 : The model perfectly explains the variability in the dependent variable. This is rare in practice and may indicate overfitting.\n",
    "\n",
    "Negative R-squared: This can occur if the model is a poor fit to the data, and the dependent variable would be better predicted by the mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb35bd1b-1e53-4c03-a496-ea1444c4172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "\n",
    "Adjusted R-squared:\n",
    " Adjusted R-squared is a modification of the regular R-squared (coefficient of determination) in linear regression models. While R-squared measures the proportion of the variance in the dependent variable that is explained by the independent variables, adjusted R-squared takes into account the number of predictors in the model, providing a more balanced evaluation of model fit, especially when adding more predictors.\n",
    "\n",
    "Calculation of Adjusted R-squared:\n",
    "  The formula for adjusted R-squared is given by:\n",
    "    Adjusted R^2 = 1-((1-R^2)(N-1)/N-P-1)\n",
    "    N - No of datapoints\n",
    "    P - No of Independent features\n",
    "    \n",
    "Differences Between R-squared and Adjusted R-squared:\n",
    "1.Consideration of Model Complexity:\n",
    "  R-squared: R-squared increases with the addition of more predictors, even if they do not significantly improve the model. It may favor overly complex models.\n",
    "  Adjusted R-squared: Adjusted R-squared penalizes the addition of unnecessary predictors, as it takes into account the number of predictors in the model.\n",
    "2.Interpretability:\n",
    "  R-squared: Higher values of R-squared are not always indicative of a better model, especially if the number of predictors is high.\n",
    "  Adjusted R-squared: Provides a more interpretable measure of model fit, considering both goodness of fit and model complexity.\n",
    "3.Range of Values:\n",
    "  R-squared: Can range from 0 to 1, where 1 indicates a perfect fit.\n",
    "  Adjusted R-squared: Can be negative and ranges from −∞ to 1, with negative values indicating a poor fit.\n",
    "4.Adjustment for Sample Size:\n",
    "  R-squared: Does not explicitly account for sample size.\n",
    "  Adjusted R-squared: Includes a correction factor for sample size (n), preventing an inflation of the metric for smaller datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343a487a-6bf9-4cf6-9b59-096944bbfecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?\n",
    "\n",
    "Adjusted R-squared is more appropriate to use when evaluating and comparing linear regression models, especially in situations where there are different numbers of predictors or when considering the trade-off between model fit and complexity. Here are scenarios in which adjusted R-squared is particularly useful:\n",
    "1.Model Comparison:\n",
    "  Adjusted R-squared is valuable when comparing models with different numbers of predictors. It penalizes models that include additional predictors that do not contribute significantly to explaining the variance in the dependent variable.\n",
    "2.Variable Selection:\n",
    "  When conducting variable selection or model building, adjusted R-squared helps in choosing a model that strikes a balance between goodness of fit and simplicity. It discourages the inclusion of unnecessary predictors that may lead to overfitting.\n",
    "3.Preventing Overfitting:\n",
    "  In situations where there is a risk of overfitting, especially when the number of predictors is close to the number of observations, adjusted R-squared provides a more conservative measure of model fit. It helps avoid selecting overly complex models that perform well on the training data but may not generalize well to new data.\n",
    "4.Comparing Models with Different Sample Sizes:\n",
    "  Adjusted R-squared accounts for sample size in its calculation, making it more appropriate when comparing models based on datasets with different numbers of observations.\n",
    "5.Controlling for Model Complexity:\n",
    "  Adjusted R-squared is useful when the goal is to control for model complexity and choose a model that provides a good balance between fit and the number of predictors. This is particularly relevant in the context of parsimony, where simpler models are preferred if they offer similar predictive performance.\n",
    "6.Multicollinearity Concerns:\n",
    "  When multicollinearity is a concern (high correlation among predictors), adjusted R-squared helps assess the model's performance by considering the effective number of independent variables.\n",
    "7.Regression with Small Sample Sizes:\n",
    "  In situations with small sample sizes, adjusted R-squared is often preferred over R-squared, as R-squared tends to be more sensitive to variations in small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ea0e5f-284e-45c8-8753-debc426ea73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?\n",
    "\n",
    "1. Mean Absolute Error (MAE):\n",
    "Calculation:\n",
    "    MAE = 1/n(∑(i=1 to n)|(actual value-predicted value)|\n",
    "              n - no of datapoints\n",
    "Interpretation:\n",
    "  MAE represents the average absolute difference between the actual and predicted values. It is less sensitive to outliers compared to other metrics like MSE.\n",
    "\n",
    "2. Mean Squared Error (MSE):\n",
    "Calculation:\n",
    "     MSE = 1/n(∑(i=1 to n)(actual value-predicted value)^2\n",
    "              n - no of datapoints\n",
    "Interpretation:\n",
    "   MSE represents the average squared difference between the actual and predicted values. Squaring the errors gives more weight to larger errors, making MSE more sensitive to outliers.\n",
    "\n",
    "3. Root Mean Squared Error (RMSE):\n",
    "\n",
    "Calculation:\n",
    "        RMSE = sqrt(MSE)\n",
    "Interpretation:\n",
    "  RMSE is the square root of MSE and provides a measure of the average magnitude of the errors in the same units as the dependent variable. It is commonly used when the errors are expected to be normally distributed.\n",
    "\n",
    "Comparison:\n",
    "MAE: Measures the average absolute deviation and is less sensitive to outliers.\n",
    "MSE: Squares the errors, giving more weight to larger errors. More sensitive to outliers.\n",
    "RMSE: The square root of MSE, providing a measure of the average magnitude of errors in the original units.\n",
    "\n",
    "Choosing the Right Metric:\n",
    "MAE: Use when outliers should have less influence on the evaluation, and you want to understand the average absolute error.\n",
    "MSE/RMSE: Use when larger errors should have a greater impact on the evaluation or when the distribution of errors is expected to be Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a3de55-7d47-4e63-9254-3cfa2098d056",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis.\n",
    "\n",
    "Advantages and Disadvantages of RMSE, MSE, and MAE in Regression Analysis:\n",
    "1. Mean Absolute Error (MAE):\n",
    "Advantages:\n",
    "  Robust to Outliers: MAE is less sensitive to outliers compared to MSE and RMSE. It gives equal weight to all errors regardless of their magnitude.\n",
    "Disadvantages:\n",
    "  Not Sensitive to Magnitude: Since MAE treats all errors equally, it may not penalize large errors enough, and it may not provide a strong incentive for the model to minimize large deviations.\n",
    "2. Mean Squared Error (MSE):\n",
    "Advantages:\n",
    "  Sensitivity to Errors: MSE gives more weight to larger errors, making it sensitive to outliers. This can be an advantage when larger errors are considered more critical.\n",
    "Disadvantages:\n",
    "  Impact of Outliers: MSE is more affected by outliers due to the squaring of errors, which can lead to an overemphasis on the impact of extreme values.\n",
    "3. Root Mean Squared Error (RMSE):\n",
    "Advantages:\n",
    "  In Same Units as Dependent Variable: RMSE is in the same units as the dependent variable, making it easily interpretable and providing a measure of the average magnitude of errors.\n",
    "Disadvantages:\n",
    "  Sensitive to Outliers: Like MSE, RMSE is sensitive to outliers due to the squaring of errors. Outliers can have a disproportionate impact on the metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a028d98-d358-4a9e-ae7e-b97fc4806d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?\n",
    "\n",
    "Lasso Regularization:\n",
    "  Lasso (Least Absolute Shrinkage and Selection Operator) regularization is a technique used in linear regression models to add a penalty term based on the absolute values of the regression coefficients. \n",
    "  It is designed to encourage sparse models by driving some of the coefficients to exactly zero, effectively performing feature selection.\n",
    "Objective Function with Lasso Regularization:\n",
    "\n",
    "The objective function to be minimized in lasso regularization is:\n",
    "   J(θ)=MSE+λ∑(i=1-n)|θ(i)|\n",
    "J(θ): The cost function \n",
    "MSE is the mean squared error.\n",
    "θ(i): The regression coefficients.\n",
    "λ: The regularization parameter that controls the strength of the penalty term.\n",
    "Differences from Ridge Regularization:\n",
    "\n",
    "While both Lasso and Ridge regularization add a penalty term to the linear regression objective function, the key difference lies in the type of penalty term:\n",
    "\n",
    "Lasso Regularization:\n",
    "  Penalty term: λ∑(i=1-n)|θ(i)| \n",
    "  Encourages sparsity by driving some coefficients to exactly zero.\n",
    " Effective for feature selection, as it tends to select a subset of the most relevant features.\n",
    "Ridge Regularization:\n",
    "   Penalty term: λ∑(i=1-n)(θ(i))^2\n",
    "   Does not drive coefficients to exactly zero but penalizes large coefficients.\n",
    "   Tends to shrink the magnitudes of all coefficients without eliminating any.\n",
    "When to Use Lasso Regularization:\n",
    "1.Feature Selection:\n",
    " When there is a large number of features, and some of them are expected to be irrelevant or redundant, Lasso can be useful for feature selection by driving some coefficients to zero.\n",
    "2.Sparse Models:\n",
    "  In situations where a simpler, more interpretable model is desired, Lasso regularization can create sparse models with fewer non-zero coefficients.\n",
    "3.Handling Multicollinearity:\n",
    "  Lasso can be effective in handling multicollinearity by selecting one variable from a group of highly correlated variables and driving the coefficients of others to zero.\n",
    "4.When a Subset of Features is Relevant:\n",
    "  When it is believed that only a subset of features is relevant for predicting the dependent variable, Lasso can be more appropriate than Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71f7355-c196-4033-bec1-84256bfc65e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate.\n",
    "\n",
    "\n",
    "Regularized linear models, such as Ridge and Lasso regression, help prevent overfitting in machine learning by adding a penalty term to the cost function that discourages the model from fitting the training data too closely. This penalty term penalizes large coefficients, which in turn limits the complexity of the model. \n",
    "Here's how regularized linear models work to prevent overfitting, illustrated with an example:\n",
    "\n",
    "Example:\n",
    "Consider a scenario where you have a dataset with a single independent variable (X) and a dependent variable (Y), and you want to fit a linear regression model. The goal is to prevent overfitting while still capturing the underlying relationship between X and Y.\n",
    "1. Simple Linear Regression:\n",
    " Y = b(o)+b(1)*X+ε\n",
    "    Y - Dependent variable\n",
    "    X - Independent variable\n",
    "    b(o) - y-intercept\n",
    "    b(1) - slope of the line\n",
    "    ε - error term\n",
    "\n",
    "In simple linear regression, the model aims to minimize the sum of squared differences between the observed and predicted values. However, in the presence of noise or outliers, the model might capture too much detail from the training data, leading to overfitting.\n",
    "\n",
    "2. Regularized Linear Regression (Ridge or Lasso):\n",
    "Ridge Regression:\n",
    "     J(θ)=MSE+λ∑(i=1-n)(θ(i))^2\n",
    "In Ridge regression, an additional penalty term is added to the mean squared error (MSE) cost function. The penalty term (\n",
    "\n",
    "Lasso Regression:\n",
    "       J(θ)=MSE+λ∑(i=1-n)|θ(i)|\n",
    "\n",
    "In Lasso regression, a different penalty term is used. This penalty promotes sparsity by driving some coefficients exactly to zero. Again, controls the strength of the penalty.\n",
    "\n",
    "Overfitting Prevention:\n",
    "Ridge and Lasso Penalties:\n",
    "   The additional penalty terms in Ridge and Lasso discourage the model from fitting the training data too closely.\n",
    "Magnitude of Coefficients:\n",
    "   The penalty terms control the magnitude of the coefficients. As the penalty increases, the model is forced to have smaller coefficients, preventing it from becoming overly complex.\n",
    "Feature Selection (Lasso):\n",
    "   Lasso, in particular, can lead to sparse models by driving some coefficients to exactly zero. This aids in feature selection, where irrelevant or redundant features are eliminated.\n",
    "    \n",
    "Hyperparameter Tuning:\n",
    "Regularization Strength (λ):\n",
    "   The hyperparameter λ is crucial for controlling the strength of regularization. Cross-validation is often used to tune λ to find the optimal balance between model fit and prevention of overfitting.\n",
    "    \n",
    "Overall Impact:\n",
    "Regularized linear models strike a balance between fitting the training data and preventing overfitting. By introducing penalties on the size of the coefficients, these models provide a more generalized solution that tends to perform well on new, unseen data.\n",
    "\n",
    "Summary:\n",
    "Simple Linear Regression: \n",
    "    Prone to overfitting, especially with noisy or complex data.\n",
    "Regularized Linear Models (Ridge and Lasso): \n",
    "    Introduce penalty terms that prevent overfitting by controlling the magnitude of coefficients and encouraging sparsity. They are particularly useful in high-dimensional datasets or when feature selection is desirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bc48b2-9a37-4dd1-ba6f-d39886f371a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis.\n",
    "\n",
    "Here are some limitations and considerations associated with regularized linear models:\n",
    "1.Loss of Interpretability:\n",
    "  Regularization penalties can lead to shrinkage of coefficients, and in the case of Lasso, some coefficients may be driven exactly to zero. While this aids in feature selection, it makes the interpretation of the model less straightforward, as the importance of individual features becomes less clear.\n",
    "2.Sensitivity to Hyperparameter Choice:\n",
    "  The performance of regularized models is highly dependent on the choice of hyperparameters, such as the regularization strength (α). Selecting an inappropriate value for the hyperparameter may lead to underfitting or overfitting.\n",
    "3.Assumption of Linearity:\n",
    "  Regularized linear models assume a linear relationship between the independent and dependent variables. If the true relationship is highly non-linear, these models may not capture the underlying patterns effectively.\n",
    "4.Impact of Outliers:\n",
    "  Regularized models can be sensitive to outliers, especially if the regularization penalty is not strong enough. Outliers may disproportionately influence the coefficients and compromise the model's performance.\n",
    "5.Limited Handling of Non-Gaussian Residuals:\n",
    "  Regularized linear models assume Gaussian (normal) distribution of residuals. If the residuals are significantly non-Gaussian, it may affect the reliability of the models and the validity of statistical inference.\n",
    "\n",
    "Considerations:\n",
    "1.Regularization vs. Simplicity:\n",
    "   The choice between regularized and non-regularized models depends on the trade-off between model complexity and simplicity. In cases where interpretability is crucial, a simpler model might be preferred.\n",
    "2.Nature of the Data:\n",
    "   Regularized models are particularly useful in high-dimensional datasets or when feature selection is important. However, in simpler datasets, the additional complexity introduced by regularization may not be necessary.\n",
    "3.Exploratory Data Analysis:\n",
    "   It's essential to conduct thorough exploratory data analysis to understand the nature of the data and whether the assumptions and characteristics of regularized linear models are appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d02c4d-d41b-464b-baf8-e36c375bd2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?\n",
    "\n",
    "The choice between Model A and Model B depends on the specific goals of your analysis and the characteristics of the data. Both RMSE (Root Mean Squared Error) and MAE (Mean Absolute Error) are commonly used metrics for evaluating regression models, and each has its own strengths and limitations.\n",
    "\n",
    "Comparing RMSE (Model A) and MAE (Model B):\n",
    "Model A (RMSE = 10):\n",
    "RMSE is sensitive to larger errors due to squaring the differences between predicted and actual values.\n",
    "It penalizes larger errors more heavily than smaller errors.\n",
    "Model B (MAE = 8):\n",
    "MAE is less sensitive to larger errors as it takes the absolute values of the differences between predicted and actual values.\n",
    "All errors are treated equally.\n",
    "\n",
    "Considerations for Model Selection:\n",
    "Magnitude of Errors:\n",
    "   If your primary concern is the magnitude of errors and you want to give equal weight to all errors, MAE (Model B) may be more appropriate. MAE is often preferred when the dataset contains outliers or when large errors should not be overly penalized.\n",
    "Sensitivity to Outliers:\n",
    "   If your dataset contains outliers and you want the metric to be sensitive to these outliers, RMSE (Model A) might be more appropriate. RMSE tends to give more weight to larger errors, making it sensitive to outliers.\n",
    "Squaring Effect:\n",
    "   The squaring effect in RMSE can make it more responsive to large errors, which may or may not align with the goals of the analysis. If you want to prioritize minimizing large errors, RMSE could be more suitable.\n",
    "Limitations:\n",
    "Context Dependence:\n",
    "  The choice between RMSE and MAE depends on the specific context of the problem, the importance of different types of errors, and the nature of the data.\n",
    "Impact of Outliers:\n",
    "  Both metrics can be influenced by outliers, but RMSE tends to be more sensitive due to the squaring of errors. It's important to consider the impact of outliers on the choice of metric.\n",
    "Interpretability:\n",
    "  RMSE is in the same units as the dependent variable, making it easily interpretable. However, in some cases, the interpretation of MAE might be more straightforward since it directly represents the average absolute error.\n",
    "Recommendation:\n",
    "  If the goal is to minimize the impact of large errors and treat all errors equally, Model B (MAE = 8) may be preferred.\n",
    "  If the analysis prioritizes sensitivity to larger errors and the dataset has outliers, Model A (RMSE = 10) might be more appropriate.\n",
    "  It's advisable to consider the characteristics of the data, the goals of the analysis, and the specific context to make an informed choice.\n",
    "\n",
    "In conclusion, the selection of the better-performing model depends on the specific considerations and objectives of the analysis. Both RMSE and MAE provide valuable insights into the model's performance, and the choice between them should align with the goals and characteristics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18593053-7e95-4f52-9635-54345dbde4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?\n",
    "\n",
    "The choice between Ridge regularization (L2 regularization) and Lasso regularization (L1 regularization) depends on the specific characteristics of the data and the goals of the analysis. Both regularization methods introduce penalty terms to the linear regression objective function to prevent overfitting, but they have different effects on the model's coefficients. Let's discuss the characteristics of each type of regularization and the potential trade-offs:\n",
    "\n",
    "Ridge Regularization (Model A - Regularization Parameter λ = 0.1)\n",
    "        J(θ)=MSE+λ∑(i=1-n)(θ(i))^2\n",
    "Effect on Coefficients:\n",
    "   Ridge regularization penalizes the sum of squared coefficients. It tends to shrink the coefficients toward zero but does not force them exactly to zero.\n",
    "   It is effective in handling multicollinearity and situations where all features may contribute to the model.\n",
    "Lasso Regularization (Model B - Regularization Parameter \n",
    "          J(θ)=MSE+λ∑(i=1-n)|θ(i)|\n",
    "\n",
    "Effect on Coefficients:\n",
    "   Lasso regularization penalizes the sum of the absolute values of coefficients. It can drive some coefficients exactly to zero, effectively performing feature selection.\n",
    "   It is useful for creating sparse models when there is a belief that some features are irrelevant or redundant.\n",
    "Model Selection Considerations:\n",
    "Feature Selection:\n",
    "    If the goal is to select a subset of important features and create a more interpretable model, Lasso regularization (Model B) might be preferred due to its ability to drive some coefficients to exactly zero.\n",
    "Handling Multicollinearity:\n",
    "    If multicollinearity is a concern, and you want to shrink coefficients without excluding any features, Ridge regularization (Model A) could be more appropriate.\n",
    "Interpretability:\n",
    "    If interpretability is a priority, Ridge regularization might be favored since it does not force coefficients to zero.\n",
    "                      \n",
    "Trade-offs and Limitations:\n",
    "Sensitivity to Hyperparameter Choice:\n",
    "   The effectiveness of Ridge and Lasso regularization depends on the choice of the regularization parameter (λ). The ideal value often needs to be determined through cross-validation.\n",
    "Loss of Interpretability (Lasso):\n",
    "    Lasso regularization may lead to sparse models, which can be advantageous for feature selection, but it makes the interpretation of individual coefficients less straightforward.\n",
    "Interaction with Correlated Features:\n",
    "    Lasso may arbitrarily select one feature over another in the case of highly correlated features, potentially leading to instability.\n",
    "Recommendation:\n",
    "  If feature selection and sparsity are crucial, and there's a belief that some features are irrelevant, Model B (Lasso regularization) might be preferred.\n",
    "  If interpretability is a priority, and multicollinearity is a concern, Model A (Ridge regularization) could be more suitable.\n",
    "  It's advisable to consider the specific goals, characteristics of the data, and the trade-offs associated with each regularization method.\n",
    "\n",
    "In conclusion, the choice between Ridge and Lasso regularization depends on the specific objectives and characteristics of the data. Each method has its strengths and limitations, and the decision should be made based on the priorities of the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4076ba-5455-4f19-bbd3-4fe568f3be48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
